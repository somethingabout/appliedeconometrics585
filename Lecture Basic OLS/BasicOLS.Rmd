---
title: "OLS Basics"
output: html_notebook
---
A good reference on basic econometrics with R can be found on [this](https://www.econometrics-with-r.org/index.html) website, where you can also do some practice exercises with R directly.

We'll be importing excel datasets, so will need to install a package to do so:

```{r}
#install.packages("readxl")
```

Now let's load the library and import the dataset
```{r}
library("readxl")
ceodata <- read_excel("/Users/mkaltenberg/Dropbox/Pace/ECO585/R notebooks/Data/excelfiles/ceosal1.xls", col_names = c("salary", "pcsalary", "sales", "roe","pcroe", "ros","indus","finance","consprod","utility","lsalary", "lsales"))
```

Let's see how to rename variables
```{r}

#library(tidyverse)
#ceosalary<- as_tibble(ceodata)

#an example on how to rename variables
#ceosalaryex <- ceosalary %>% 
  #rename(
   # inc = salary,
    #returnexp = roe
#    3)
```
Let's estimate a simple regression:

\begin{gather*}\widehat{s a l a r y} =\beta_0 +\beta_1\text{}r o e \end{gather*}

where annual CEO salary is in thousands of dollars and the return on equity is a percent.

Let's run that regression in R:
```{r}
library(wooldridge)
data("ceosal1")
roe_reg <- lm(salary ~ roe, data = ceosal1)
summary_roe <-summary(roe_reg)
summary_roe
```

How do you interpret this coefficient?

A one percentage point increase in $r o e$ increases predicted salary by $18.501$, or $18,501

Remember that we are trying to understand differences in one unit. A change in one unity in roe is percentage points. Percent change is a ratio - it measures a rate of change. Take this example, the percent of people on poverty in the USA in 2015 was 13.5% and in 2020 it is 11.3%. The the poverty rate between 2015 and 2020 is -2.2 percentage points.  However, the percent change of poverty between 2015 and 2020 is -12.6% ($\frac{13.5-11.3}{13.5}$). The poverty rate decreased by 12.6%. Percent is a number expressed as a fraction of 100. So, the percent of people in poverty is a reflection of $\frac{N. Poverty}{Tot. Population}$.

Take this example: 

What if we measure $r o e$ as a decimal, rather than a percent? Define 

\begin{equation*}r o e d e c =r o e/100
\end{equation*}

What will happen to the intercept, slope, and $R^{2}$ when we regress 

\begin{equation*}s a l a r y\quad \text{on}\quad r o e d e c\text{?}
\end{equation*}

Nothing should happen to the intercept: $r o e d e c =0$ is the same as $r o e =0$. But the slope will increase by 100. The goodness-of-fit should not change, and it does not.

```{r}
ceosal1 <- ceosal1 %>%
  mutate(roedec= roe/100)

roedec<-lm(salary ~ roedec, data =ceosal1)

roedec_sum <- summary(roedec)
roedec_sum
```
Now a one percentage point change in $roe$ is the same as $ \Delta r o e d e c =.01$, and so we get the same effect as before.

What if we measure salary in dollars, rather than thousands of dollars? 

Both the intercept and slope get multiplied by 1,000


# OLS properties
Let's see some of the OLS properties in action

1) The sum of residuals will equal to 0
Let's see it:
```{r}
u <- resid(lm(salary ~ roedec, data =ceosal1))
sum(u)
```
There are some rounding errors in computation, but the sum is close to 0

2) We know that $\bar{y}=\bar{\hat{y}}$

Let's show it:
```{r}
yhat <- predict(lm(salary ~ roedec, data =ceosal1))

mean(ceosal1$salary)
mean(yhat)
```

The sample covariance (and therefore the sample correlation) between the explanatory variables and the residuals is always zero:

```{r}
sum(ceosal1$roedec*u)
```
Again, there are some rounding errors in computation, but the sum is close to 0


Because the $\hat{y}_i$ are linear functions of the $x_i$, the fitted values and residuals are uncorrelated, too:
```{r}
sum(yhat*u)
```
Again, there are some rounding errors in computation, but the sum is close to 0

3) The point ($\hat{x}, \hat{y}$) is always on the OLS regression line. That is, if we plug in the average for x, we predict the sample average for y:
$\bar{y} = \beta_0 + \beta_1\bar{x}̄$

```{r}
mean(ceosal1$roedec)
mean(ceosal1$salary)
```
We can plot our regression 


And we can graph our residuals like this:
```{r}
plot(salary ~ roedec, data=ceosal1)
abline(lm(salary ~ roedec, data =ceosal1))
points(mean(ceosal1$roedec), mean(ceosal1$salary), col = "red")
```

## Simple Linear Regression Assumptions

Recall that for the Simple Linear Regression the assumptions are:
  
  1. Linear in parameters. x and u are random, and thus, so is y.
  
  2. Random sample size of n
  
  3. Sample Variation in the Explanatory Variable (you need to have variance in X - it can't be a constant)
  
  4. Zero Conditional Mean (error term has zero mean given any value of the explanatory variable) $E(u|x)=0$ for all x
  
  5. Homoskedasticity or Constant Variance (error term has the same variance given any value of the explanatory variable x, $Var(u|x) = \delta^2 > 0$ for all x

We mostly work in the realm of multiple linear regressions and the difference between the simple linear regression and multiple linear regressions requires only one additional assumption, so we'll just hop right into it.

## Multiple Linear Regression Assumptions

1. $y=\beta_0 + \beta_1x_1+\beta_2x_2 + \dots + \beta_kx_k + u$ (linearity)

2. Random sampling from the population

${(u_i,X_i), i =1, \dots, N}$ is a random sample from the $(u, X)$ population where  $(u_i, X_i)$ are i.i.d. (independently and identically  distributed)

3. no perfect collinearity in the sample (aka non redundancy)

$\sum^n_{i=1} (X_i-\bar{X})^2>0$

4. $E(u|x_1,\dots,x_k) = E(u) = 0$ (zero conditional mean)

5. $Var(u|x_1, \dots, x_k) = Var(u) = \sigma^2$ (homoskedasticity)

6. $u_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(0,\sigma^2_u)$
The distribution u given $(x_1, \dots, x_k$ is $Normal(0,\sigma^2)$  (Normality of the error term)

Assumptions 1 - 4 ensure unbiasedness in our parameter estimation aka $E(\hat{\beta_j})=\beta_j$

Assumptions 1-5 are the Gauss-Markov assumptions aka BLUE

Assumption 1-6 are the classic linear assumptions and do exact statistical inference.

Sometimes, you'll see the conditional expectation function written as:

$E(Y_i|X_i) = \beta_0 + \beta_1X_i$

This implies:

i. The conditional expectation of $Y_i$ is linear in $X_i$ (Asmp. 1)
ii. the error term is mean-independent of the regressor (Asmp. 4)
iii. the error term is distribution around zero.
(Asmp. 5)

Assumptions 1, 5 and 6 are easier to relax. Assumption 2 is easy to implement and assumption 3 is data dependent, but will be important for panel data.
Assumption 4 - which we've discussed a lot, is what a lot of econometrics is all about. 

We can write assumptions 1,3,4,5 concisely:

$Y_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(\beta_0 +\beta_1X_i, \sigma^2)$

or

$u_i | X_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2_u)$


### Perfect Collinearity (Assumption 3)
Assumptions 1,2, 4, and 5 are the same as in the simple linear model. The new assumption is about perfect collinearity. When estimating OLS, perfect linear combinations can't be distinguished. In other words, we can't distinguish the estimates from the two variables and there are infinite possible answers. 

This seems obvious in cases where we have a set of categorical dummies. The solution to this issue is to drop a variable, and as consequence, our interpretation changes. We interpret a set of dummies relative to the one we dropped.  

Take this example, where we measure the impact of region on wages. We interpret the coefficient of south relative the variable excluded (east). Including all variables would lead to perfect collinearity.


```{r}
#install.packages("wooldridge")
library(wooldridge)
data("wage1")

dummies <- lm(lwage ~ educ+tenure+smsa+northcen+south+west, data= wage1)

summary(dummies)
```

The problem gets harder if for some reason you have perfect linear combinations of other variables (that you didn't realize). This is why it's good to do correlation matrices at the start of your research, so you can see if there are variables that are perfectly correlated (and also to check for severity of multi-collinearity, which we will get to later).

Consider non-obvious example - imagine that you have information about wages, industry and union rates for Sweden. You want to understand how industry and union participation impacts average earnings $ln(wage_i) = \beta_0 + \beta_1 Union_i + \sum_j^i\beta_j Ind_i +u_i$ where j represents the number of industry dummies and union is a dummy variable on whether an individual has a union contract or not. If one industry, let's say manufacturing, has every employee under a union contract, this will result in perfect collinearity. This is because Industry Manuf and Union are perfectly collinear - OLS can't solve the estimate for manuf industry and union as there are infinite number of solutions. This example is something where you will have to drop either the industry manuf. or union dummy. 

So, we see what happens when assumption 3 breaks. R may automatically drop a perfectly collinear variable, but if it drops without you realizing why, you should review your data. In practice, this is something that you may make a mistake and R fixes it for you, but assumptions 4 and 5 are much more nuanced. Assumption 5 is an assumption we can often "relax" by correcting our standard errors. Assumption 4 is the trickiest assumption to fix, and one that econometricians and applied econometricians spend a lot of time thinking about. 

As a side note, assumption 1 is also something that econometricians work on - particularly when parameters can behave non-linearly or when the dependent variable has a non-normal distribution. 

### Zero-conditional mean (Assumption 4)

This assumption is by far the most important assumption - if this is broken, then we have a problem with biasedness. Bad news is that this assumption is broken frequently. 

It is the glue of showing causality.

It's easiest to think of this in a randomized setting or a randomized control trial. 

We want to evaluate the effectiveness of health care on health outcomes. In an ideal world, we'd create two groups of identical people. Where we'd have one twin group that is exposed to health care and the other twin group that does not have health care - and this assignment is *random*. Then we'd compare their health outcome differences - a difference in the means of the two groups. 

Obviously, we don't have identical twins in every possible way roaming around. So, the best we an do is a build an ideal setting where we have nearly identical groups.  Any setting in which the two groups that we are comparing are non-randomly different are what makes up our error term. 

So, you can imagine, that there are tons of settings in which the groups we may differ - and those situations will break assumption 4. 

Econometrics is mostly building an statistically ideal situation in which we have two identical groups and the only difference between the two groups are randomly assigned - that is your X and a variable that you are interested in understanding. 

And when you consider your research, your interest should focus on a couple of X's (meaning that you want to create an ideal setting for a couple of variables - you want to understand unbiased estimates for a one or a couple variables).  The other variables that you include - those controls - are meant to help you create this ideal setting, but often, they can biased (and that's ok - so long as you understand the caveats and you don't interpret those coefficients as causal).

### Policy Analysis
For a moment, allow an important tangent about causal effects.

Often, you want to evaluate a policy or intervention that has two states in the world. In data terms, it's a binary terms. In real world terms, one group was exposed the the intervention (this is called the *treatment* group) and the other group was not exposed (this is called the *control* group). Language is important as we will return to using the term treatment group very frequently. 

We know the *outcome* of these states - that somebody has either been part of the policy group or not. For example, they received free health insurance or they did not. 

The treatment effect is simple
$$te = y(1) - y(0)$$

$y(1)$ being that they got the treatment and $y(0)$ that they did not.  You should be asking yourself, but wait - they only participated in one option. They can't do both, so how can you subtract the different between participating and not participating of the same person?!

We need to rely on counterfactuals. Holding everything constant, what is the effect of health if they receive free health care. HOlding everything constant, what is the effect of health if they don't receive the benefit?  These are the potential outcomes (or counterfactual outcomes)

So, we often can't estimate each person's treatment effect. INSTEAD, we estimate the **average treatment effect (ATE henceforth)** This is the average of the treatment effects across the entire population.

$$\tau_{ATE} = E[te_i] = E[y_i(1)-y(0_i)]$$
And because of the linearity of the expected value this is equal to:
$$= E[y_i(1)]-E[y_i(0)]$$

The observed outcome of $y_i$ can be

$$y_i= (1-x_i)y_i(0)+x_iy_i(1)$$
Where $y_i = y_i(0)$ if $x_i = 0$ and $y_i = y_i(1)$ if $x_i=1$

We can rearrange:

multiply out  $y_i = y_i(0)+y_i(0)x_i+x_iy_i(1)$ 

rearrange and get:  $$y_i = y_i(0) + [y_i(1) - y_i(0)]x_i$$

Now impose a pretty unrealistic assumption of constant treatment effects for all i

$y_i(1)=\tau +y_i(0)$  which is also $\tau = y_i(1)-y_i(0)$

Plug that into our equation of y_i and it simplifies to:
$y_i = y_i(0) + \tau x_i$

If we define $y_i(0) = \alpha_0 +u_i(0)$ (and recall that $\alpha_0 = E[y_i(0)]$ and as usual, the error term is expected value is zero $E[u_i(0)=0$]). Plug this into our equation for $y_i$ (replace $y_i(0)$):

$y_i = \alpha_0 + \tau_ix_i + u_i(0)$

If we define $\beta_0 = \alpha_0, \beta_1 = \tau$ and $u_i = u_i(0)$, we get the usual:

$$y_i = \beta_0 + \beta_1x_i + u_i$$
$\beta_1$ is just the treatment or causal effect.  This is often called the difference in means estimator and is the unbiased treatment effect $\tau$. If $x_i$ is independent of $u_i(0)$ then we know $E(u_i(0)|x_i] = 0$). This is the same as saying $x_i$ is independent of $y_i(0)$. This assumption is guaranteed only under *random assignment*.   This means that the treatment must be assigned randomly.

The above equation is an RCT (randomized control trial).

Suppose we relax the assumption of constant treatment effect. We can write the individual treatment effect as
$$te_i = y_i(1)-y_i(0)=\tau_{ATE}+[u_i(1)- u_i(0)]$$
recall where $y_i(1) = \alpha_1+u_i(1)$ and $\tau_{ATE} = \alpha_1 = \alpha_0$ \

$\tau_{ATE}$ is the average across the entire population and $u_i(1) - u_i(0)$ is the deviation from the population average for unit i.

Plug $te_i = y_i(1)-y_i(0)=\tau_{ATE}+[u_i(1)- u_i(0)]$ into 
$y_i = y_i(0) + [y_i(1) - y_i(0)]x_i$ and get:

$$y_i = \alpha_0 + \tau_{ATE}x_i + u_i(0)+[u_i(1)-u_i(0)]x_i = \alpha_0 + \tau_{ATE}x_i+u_i$$

and the error term is $u_i(0)+[u_i(1)-u_i(0)]$

random assignment assumption is now that x_i is independent of [u_i(0),u_i(1)], even thought $u_i$ depends on $x_i$

$$E(u_i|x_i) = E[u_i(0)|x_i] + E[u_i(1)]-u_i(0)|x_i]x_i$$
*note that the error term is not independent of $x_i$, but this isn't a problem for showing our unbiased estimator.  This makes sense because the variance should change depending if you were treated or untreated. These results only hold because of random assignment.


Let's see what happens when we don't have random assignment (as it's very hard to have this situation without doing an experiment). Instead, let's imagine if we can have control variables that help predict the potential outcomes and determine the assignment in the treatment and control group - let's call this group of controls x.

To reduce confusion we will rewrite $y_i= (1-x_i)y_i(0)+x_iy_i(1)$ as 

$$y_i= (1-w_i)y_i(0)+w_iy_i(1)$$
Then we can have the assumption that 
$w$ is independent of $[y(0),y(1)]$ *conditional* on x

This is called **conditional independence** where the variables in x are in the conditioning set. This is also called **unconfounded assignment** or **ignorable assignment**

Let's see this difference in an example of job training - you'll see this example at various points in the course.
```{r}
data("jtrain98")

simplediff <- lm(earn98 ~ train, data = jtrain98)
summary(simplediff)

 conditionalx<- lm(earn98 ~ train+earn96+educ+age+married, data = jtrain98)
 
 summary(conditionalx)

```

We can see that the effect of job training actually increases when we control for other factors. Without controls, the job training program  decreases income by \$2,005, but when we add controls we can see that the job training programs increase by \$2,410 The controls serve to create a kind of random assignment. As long as we include all of the variables that could effect assignment of job training. 

So, when we think of control variables, what we want to do is to be able to explain all reasons as to why some people received the treatment and why other people didn't. 

Another way to think about this is that we are trying to create an experiment where one group received treatment and the other didn't. But, the two groups are identical.  The controls are ensuring that the treatment effect is unbiased, ceteris paribus - holding all other factors constant. If we hold all factors that differentiate the two groups constant, we can say something about the treatment effect. 



### Unbiasedness of Beta 
The key assumption that you must think about the most is if assumption 4 is met. This assumption is often broken, and as a consequence will result in biasedness in your $\beta$ parameters.

Technically, assumptions 1-4 are required for unbiasedness, but in practical terms, often assumption 4 is biggest concern.  When assumption 1 is broken, we use non-linear estimation techniques that's outside of the scope of this course. Assumption 2 is a concern when you choose your data (target population), complex survey designs, designing randomized control trials, and time series data (we'll talk about this later with panel data).

We can do an experiment to see if $\beta_0$ and $\beta_1$ are unbiased when assumptions 1-4 hold. 

We will randomly draw from a normal distribution for u and x. Then we will create a y variable as a combination of x and u, specifically $y=3+2x+u$.This way will know that $\beta_0$ should be 3 and $\beta_1$ should be 2. We'll repeat this process multiple times and will see that the estimates should be close to our expected values.

```{r}
x = 3*rnorm(250)
u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)

u = 6*rnorm(250)
y = 3+2*x+u
lm(y~x)
```
You can see that the values are note exact, but they are close to expected values $\beta_1=2, \beta_0=3$. If we average these values after many random samples, we should get the true population estimate.

We won't ever know the true population value, but we hope that our sample is “typical” and produces a slope estimate close to $\beta_1$, but we can never know. 


### Omitted Variable Bias
There are many ways to break assumption 4, omitted variable bias is just one example - so, let's dive into this particular violation of assumption 4.

#### Theory 

We require two conditions for the coefficient on $X_1$ to "suffer" from omitted variable bias.

1. $X_1$ is correlated with the omitted variable ($X_2$)
2. The omitted variable ($X_2$) is a determinant of the outcome ($Y$)

If omitted variable bias is present then $\hat{\beta}_1$ does not converge (in probability) to the true value but instead to the following:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \rho_{X_1 u} \frac{\sigma_u}{\sigma_{X_1}}$$
Where $\rho_{X_1 u}$ is the correlation between the error term ($u$) and $X_1$, $\sigma_u$ is the std dev of the error term and $\sigma_{x_1}$ is the std dev of $x_1$
Quite generally, this means that there is something in the error term ($u$) that is correlated with both $X_1$ and $Y$, which invalidates the zero conditional mean assumption and biases the coefficient of $X_1$ in a particular direction.

#### Deriving the formula

The OLS estimator of the slope in a bi-variate regression:

$$ \hat{\beta_1} = \beta_1 + \frac{\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)u_i}{\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)^2} $$

The nominator of the second term converges in probability to the covariance of $X_i$ and $u_i$, so that $\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)u_i \buildrel p \over \longrightarrow  \text{cov}(X_{1i}, u_i) = \rho_{X_1 u} \sigma_u \sigma_{X_1}$. The denominator converges in probability to the variance of $X_{1i}$, so that $\frac{1}{n}\sum_i (X_{1i} - \bar{X}_1)^2 \buildrel p \over \longrightarrow \text{var}(X_{1i}) = \sigma_{X_1}^2$. As a result, the entire equation converges to

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\text{cov}(X_{1i}, u_i)}{\text{var}(X_{1i})} $$
Or, written differently, from reshuffling the definition of a correlation, i.e. $\rho_{X_{1} u} = \text{cov}(X_{1i},u) / (\sigma_{X_1} \sigma_u)$, we have:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\rho_{X_1 u} \sigma_u \sigma_{X_1}}{\sigma_{X_1}^2} = \beta_1 + \rho_{X_1 u}\frac{ \sigma_u}{\sigma_{X_1}} $$ where the last equality follows simply from canceling terms.

If the OLS assumptions are satisfied, the last term becomes zero and our OLS estimator of $\beta_1$ is unbiased and consistent. However, if there are important omitted variables, then the OLS of $\beta_1$ is biased and inconsistent (i.e. even with large N the bias does not vanish).


#### Deriving the direction (formally)

Suppose you have two models, one simple bivariate OLS (aka the "wrong" model) 
$$ Y_i = \beta_0 + \beta_1 X_{1i} + u_i $$
and the "true" model, in which two X's are included
$$ Y_i = \gamma_0 + \gamma_1 X_{1i} + \gamma_2 X_{2i} + e_i $$
We want to know the direction of the bias in $\hat{\beta}_1$ based on the first (wrong) model.

If we look at the formula for the OV-bias, we cannot directly compute $\rho_{X_1 u}$ -- the correlation between $X_{1i}$ and the error ($u_i$) -- because the population error is not observed. However, we can clearly see that the "true" model is different from the biased model in terms of parameters and in the sense that $u_i = \gamma_2 X_{2i} + e_i$. Further, since the second model is the true model, we have $\text{E}[e_i | X_{1i}, X_{2i}]=0$; that is, the expectation of $e_i$ will be zero and the error is not systematically correlated with $X_{1i}$.

Now we can rewrite the covariance between $X_{1i}$ and the error term ($u_i$) in terms of observables using the alternative formulation from above for simplicity:

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\text{cov}(X_{1i}, u_i)}{\text{var}(X_{1i})} = \beta_1 + \frac{\text{cov}(X_{1i}, [\gamma_2 X_{2i} + e_i])}{\text{var}(X_{1i})} $$

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 + \frac{\gamma_2\text{cov}(X_{1i}, X_{2i}) + \text{cov}(X_{1i}, e_i) }{\text{var}(X_{1i})} =  \beta_1 + \frac{\gamma_2\text{cov}(X_{1i}, X_{2i}) + 0 }{\text{var}(X_{1i})}$$

which leads us to the last version[^2]

$$\hat{\beta}_1 \buildrel p \over \longrightarrow  \beta_1 +  \frac{\gamma_2\text{cov}(X_{1i}, X_{2i})}{\text{var}(X_{1i})} = \beta_1 + \gamma_2 \rho_{X_{1} X_{2}} \frac{\sigma_{X_2}}{\sigma_{X_1}} $$

We do not need to know the ratio of the two standard deviations ($\frac{\sigma_{X_2}}{\sigma_{X_1}}$) to derive the direction, as it will always be positive, so all we care about is $\gamma_2 \rho_{X_{1} X_{2}}$. We can simply estimate $\gamma_2$ by running the second model and similarly just estimate the correlation $\rho_{X_{1} X_{2}}$. All that's left is to figure out the direction of the true coefficient. We can determine the sign of the "true" $\beta_1$  by estimating its counterpart $\gamma_1$. Then we have

1. if $\beta_1 < 0$ and $\gamma_2 \rho_{X_{1} X_{2}}< 0$, then $\beta_1$ is downward biased and overestimated.
2. if $\beta_1 < 0$ and $\gamma_2 \rho_{X_{1} X_{2}}> 0$, then $\beta_1$ is upward biased and underestimated.
3. if $\beta_1 > 0$ and $\gamma_2 \rho_{X_{1} X_{2}}< 0$, then $\beta_1$ is downward biased and underestimated.
4. if $\beta_1 > 0$ and $\gamma_2 \rho_{X_{1} X_{2}} > 0$, then $\beta_1$ is upward biased and overestimated.

A simple way to remember this is:

|     | $Corr(x_1,x_2)>0$ | $Corr(x_1,x_2)<0$ |
|-----|-------------------|--------------------|
| $Corr(y,x) > 0$| pos bias | neg bias | 
| $Corr(y,x) > 0$| pos bias | neg bias | 

This works well only if we compare a single covariate model to a model with one additional X. In practice, there may be multiple omitted variables at work, each possibly biasing the true coefficient of $X_1$ in different directions. Theoretically, we can even envision a case where there are two omitted variables in opposing directions with offsetting effects!

[^2]:In the last equality we use the same trick as applied by S&W when deriving the original formula. Note that $\rho_{X_{1} X_{2}} = \text{cov}(X_{1i},X_{2i}) / (\sigma_{X_1} \sigma_{X_2})$. Reshuffling and then canceling terms gives the last expression.

Now, let's take a practical example of Omitted Variable Biasedness.

There is a package that contains all of Wooldridge's data, so let's make our life easy:
```{r}
#install.packages("wooldridge")
```

```{r}
library(wooldridge)
```

Load in the education data (you can load data into R from the package directly)
```{r}
wagedata <-data("wage1")
biased <- lm(lwage ~ educ+tenure, data= wage1)
summary(biased)

less_biased <-lm(lwage ~ educ+exper+tenure, data= wage1)
summary(less_biased)
```

We can see that the coefficient of education changes when we compare our regression with (.0865) and without experience (.097).  It increases by ~10% - meaning that without the inclusion of experience, education was downwardly biased. 

### A note about Adjusted R-squared

The Sum of Squares decomposition still holds with $K$ regressors \vspace{-1em}

$$ \sum_{i=1}^N (Y_i - \bar{Y})^2 = \sum_{i=1}^N (\hat{Y}_i - \bar{Y})^2 + \sum_{i=1}^N \hat{u}_i^2 $$ 

However, the $R^2$ keeps going up when we add (even meaningless) regressors. So we introduce a penalty  \vspace{-1em}

This is the Adjusted $R^2$
$$\bar{R}^2 \equiv 1 - \left(\frac{N-1}{N-K-1}\right) \frac{\sum_{i=1}^N \hat{u}_i^2}{\sum_{i=1}^N (Y_i - \bar{Y})^2}$$

Similarly, the $SER$ now needs to be adjusted for the extra degrees-of-freedom used during the estimation \vspace{-1em}

$$SER \equiv s_{\hat{u}} = \sqrt{\frac{1}{N-K-1}\sum_{i=1}^N \hat{u}_i^2}$$
Can the adjusted $R^2$ decrease if we include a new variable? Depends if it is relevant or not.

What happens if we include an irrelevant variable? Let's see an example...

```{r}
mlb <-lm(lsalary ~years + gamesyr+bavg + hrunsyr+rbisyr, data = mlb1)

mlb2 <-lm(lsalary ~years + gamesyr+bavg + hrunsyr+rbisyr+percwhte, data = mlb1)
summary(mlb)
summary(mlb2)


```

When more regressors are added, SSR falls, but so does $d f =n -k -1$. $\bar{R}^{2}$ can increase or decrease. 

For $k \geq 1$, $\bar{R}^{2} <R^{2}$ unless $S S R =0$ (not an interesting case). In addition, it is possible that $\bar{R}^{2} <0$, especially if $d f$ is small. Remember that $R^{2} \geq 0$ always.

More algebraic facts: 

1. If a single variable is added to a regression, $\bar{R}^{2}$ increases if and only if the absolute $t$ statistic of the new variable is greater than one. (Note this value does not correspond to any commonly used critical value.)


2. If two or more variables are added to a regression, $\bar{R}^{2}$ increases if and only if the $F$ statistic for joint significance of the new variables is greater than one. (Again, the value one is a curiosity; it does not correspond to commonly used critical values for $F$ tests.)

Sometimes useful to write:

\begin{equation*}\bar{R}^{2} =1 -(1 -R^{2}) \frac{(n -1)}{(n -k -1)}
\end{equation*}

Important: In the $R$-squared form of the $F$ statistic that we covered, it is the usual $R$-squared, not the adjusted $R$-squared, that appears. 

Sometimes $\bar{R}^{2}$ is called the ``corrected $R$-squared,'' but this name is problematic. It implies that $\bar{R}^{2}$ corrects some statistical deficiency that $\bar{R}^{2}$ has for estimating $\rho ^{2}$. But like $R^{2}$, $\bar{R}^{2}$ is biased for $\rho ^{2}$.


### Using Adj. R-squared to choose between models

Typically, if you tell me that you chose one model over the other because it has a higher Adj. R-square, I'm going to be *VERY* skeptical. Blindly using R-square to determine the "best" model is a big no-no. 

However, there are situations where it may be helpful.

When you are trying to figure out which proxy is better than another, it might be useful.

OR maybe you want to consider different functional forms (Do I use a log or a polynomial? - both represent diminishing returns)

This is what wooldridge calls comparing *nonnested models*.

NOTE that it is only possible to do these comparisons when the dependent model remains the same. You can't use adjusted R-square to figure out if log(y) or y is the better option. 








